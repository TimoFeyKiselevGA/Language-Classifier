{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JazykovaClasifikace.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUvG4isjFcXqQi+b/FzYTa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TimoFeyKiselevGA/2020-e4-kiselev-Language-Classifier/blob/main/JazykovaClasifikace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-mN8Xc3iIVT"
      },
      "source": [
        "# **Part 1, downloading and extracting texts** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1lk-zmwiqfz"
      },
      "source": [
        "## Imports and"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YOojf48KxCQ",
        "outputId": "38daa121-1ba3-46e2-b111-2e2acd678fcc"
      },
      "source": [
        "import xml.etree.ElementTree as etree\n",
        "import codecs\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "%load_ext google.colab.data_table\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ey-XvldiyLr"
      },
      "source": [
        "## Downloading texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRu5E6J63wnX"
      },
      "source": [
        "languages_to_download = {\n",
        "  \"CZ\": True,\n",
        "  \"RU\": True,\n",
        "  \"FR\": True,\n",
        "  \"DE\": True,\n",
        "  \"EN\": True,\n",
        "  \"PL\": True,\n",
        "  \"IT\": True,\n",
        "  \"JA\": True,\n",
        "  \"UK\": True,\n",
        "  \"AR\": True,\n",
        "  \"FI\": True,\n",
        "  \"BG\": True,\n",
        "}\n",
        "\n",
        "languages_links = {\n",
        "    \n",
        "    \"CZ\": \"https://dumps.wikimedia.org/cswiki/20210120/cswiki-20210120-pages-articles.xml.bz2\",\n",
        "    \"RU\": \"https://dumps.wikimedia.org/ruwiki/20210101/ruwiki-20210101-pages-articles-multistream5.xml-p3835773p5335772.bz2\",\n",
        "    \"FR\": \"https://dumps.wikimedia.org/frwiki/20210101/frwiki-20210101-pages-articles-multistream2.xml-p306135p1050822.bz2\",\n",
        "    \"DE\": \"https://dumps.wikimedia.org/dewiki/20210101/dewiki-20210101-pages-articles-multistream6.xml-p9261245p10761244.bz2\",\n",
        "    \"EN\": \"https://dumps.wikimedia.org/enwiki/20210101/enwiki-20210101-pages-articles-multistream9.xml-p2936261p4045402.bz2\",\n",
        "    \"PL\": \"https://dumps.wikimedia.org/plwiki/20210101/plwiki-20210101-pages-articles-multistream4.xml-p1199519p2047892.bz2\",\n",
        "    \"IT\": \"https://dumps.wikimedia.org/itwiki/20210101/itwiki-20210101-pages-articles-multistream4.xml-p2206775p3593336.bz2\",\n",
        "    \"JA\": \"https://dumps.wikimedia.org/jawiki/20210101/jawiki-20210101-pages-articles-multistream5.xml-p1721647p2807947.bz2\",\n",
        "    \"UK\": \"https://dumps.wikimedia.org/ukwiki/20210120/ukwiki-20210120-pages-articles-multistream4.xml-p987986p1674442.bz2\",\n",
        "    \"AR\": \"https://dumps.wikimedia.org/arwiki/20210120/arwiki-20210120-pages-articles-multistream4.xml-p2482316p3982315.bz2\",\n",
        "    \"FI\": \"https://dumps.wikimedia.org/fiwiki/20210120/fiwiki-20210120-pages-articles-multistream.xml.bz2\",\n",
        "    \"BG\": \"https://dumps.wikimedia.org/bgwiki/20210120/bgwiki-20210120-pages-articles-multistream.xml.bz2\",\n",
        "}\n",
        "\n",
        "languages = ['CZ', 'RU', 'FR', 'DE', 'EN', 'PL', 'IT', 'JA', \"UK\", \"AR\", \"FI\", \"BG\"]\n",
        "languages_to_process = {\n",
        "  \"CZ\": True,\n",
        "  \"RU\": True,\n",
        "  \"FR\": True,\n",
        "  \"DE\": True,\n",
        "  \"EN\": True,\n",
        "  \"PL\": True,\n",
        "  \"IT\": True,\n",
        "  \"JA\": True,\n",
        "  \"UK\": True,\n",
        "  \"AR\": True,\n",
        "  \"FI\": True,\n",
        "  \"BG\": True,\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiFuGtTTQ042"
      },
      "source": [
        "def rename(s):\n",
        "    return languages_links[s].split(\"/\")[len(languages_links[s].split(\"/\"))-1].split(\".\")[0]+ \".\" +languages_links[s].split(\"/\")[len(languages_links[s].split(\"/\"))-1].split(\".\")[1]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw3Q5CnA5G3f",
        "outputId": "22727eda-425f-4d33-d73c-05c6b57f8639"
      },
      "source": [
        "for lang in languages:\n",
        "    if languages_to_download[lang]:\n",
        "        print(\"downloading \" + lang)\n",
        "        os.system(\"wget \" + languages_links[lang])\n",
        "        print(\"extracting \" + lang)\n",
        "        os.system(\"bzip2 -d ./ \" + languages_links[lang].split(\"/\")[len(languages_links[lang].split(\"/\"))-1])\n",
        "        os.system(\"mv \" + rename(lang) + \" \" + languages_links[lang].split(\"/\")[len(languages_links[lang].split(\"/\"))-1].split(\".\")[0] + \".xml\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading CZ\n",
            "extracting CZ\n",
            "downloading RU\n",
            "extracting RU\n",
            "downloading FR\n",
            "extracting FR\n",
            "downloading DE\n",
            "extracting DE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuco0Gr-jtNW"
      },
      "source": [
        "## Creating pkl files with texts without most special chars. from xmls."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOPR-DtIdlJR"
      },
      "source": [
        "def strip_tag_name(t):\n",
        "    idx = k = t.rfind(\"}\")\n",
        "    if idx != -1:\n",
        "        t = t[idx + 1:]\n",
        "    return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JuCrnVngyfE"
      },
      "source": [
        "def articles_to_csv(totalCount):\n",
        "    with codecs.open(pathTexts, \"w\", ENCODING) as textsFH:\n",
        "        textWriter = csv.writer(textsFH, quoting=csv.QUOTE_MINIMAL)\n",
        "        textWriter.writerow(['text'])\n",
        "        for event, elem in etree.iterparse(pathWikiXML, events=('start', 'end')):\n",
        "            tname = strip_tag_name(elem.tag)\n",
        "            if event == 'start':\n",
        "                if tname == 'page':\n",
        "                    title = ''\n",
        "                    id = -1\n",
        "                    redirect = ''\n",
        "                    inrevision = False\n",
        "                    ns = 0\n",
        "                    text = ''\n",
        "                elif tname == 'text':\n",
        "                    text = elem.text\n",
        "                    textWriter.writerow([text])\n",
        "                elif tname == 'page':\n",
        "                    totalCount += 1\n",
        "                if totalCount > 1 and (totalCount % 100000) == 0:\n",
        "                    print(\"{:,}\".format(totalCount))\n",
        "            elem.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip12b-8RUsLV"
      },
      "source": [
        "for lang in languages:\n",
        "    if languages_to_process[lang]:\n",
        "        print(lang)\n",
        "        PATH_WIKI_XML = './'\n",
        "        FILENAME_WIKI = languages_links[lang].split(\"/\")[len(languages_links[lang].split(\"/\"))-1].split('.')[0] + '.xml'\n",
        "        texts_template = 'temp.csv'\n",
        "        ENCODING = \"utf-8\"\n",
        "        pathWikiXML = os.path.join(PATH_WIKI_XML, FILENAME_WIKI)\n",
        "        pathTexts = os.path.join(PATH_WIKI_XML, texts_template)\n",
        "        totalCount = 0\n",
        "        articleCount = 0\n",
        "        title = None\n",
        "        articles_to_csv(totalCount)\n",
        "        a = pd.read_csv('temp.csv')\n",
        "        a = a.dropna()\n",
        "        a['text'] = a['text'].str.replace(',|\\$|\\{|\\}|\\-|\\+|\\[|\\]|\\\\n|\\<|\\>|\\\"|\\=|\\:|\\;|\\*|\\||\\#|\\$|\\(|\\)|\\/|\\.|\\–|\\_|\\«|\\»|\\—|\\!|\\&|\\?|\\`|\\~|\\%|\\@|\\¡|\\™|\\£|\\¢|\\∞|\\§|\\¶|\\•|\\ª|\\º|\\–|\\≠|\\«|\\æ|\\÷|\\≥|\\≤|\\÷|\\»|\\\\|\\^|\\ˆ|\\|\\\\|\\||\\）|\\（|\\、|\\'', ' ')\n",
        "        a['text'] = a['text'].str.replace('\\d+', '')\n",
        "        a['text'] = a['text'].str.replace('\\s\\s+' , ' ')\n",
        "        a = a.drop([a.index[0]])\n",
        "        a = a.reset_index(drop=True)\n",
        "        a['text'] = a['text'].str.lower()\n",
        "        a.to_pickle(\"./texts_\" + lang +  \".pkl\")\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY2d3zSHC85E"
      },
      "source": [
        "%reset -f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-x6PwcnkRXa"
      },
      "source": [
        "# **Part 2: Dataset preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMSTAFOOknpp"
      },
      "source": [
        "## Imports and settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKHpKVe21ZzK"
      },
      "source": [
        "import xml.etree.ElementTree as etree\n",
        "import codecs\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "%load_ext google.colab.data_table\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "import numpy as np\n",
        "import random\n",
        "#random seeds\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "languages = ['CZ', 'RU', 'FR', 'DE', 'EN', 'PL', 'IT', 'JA', \"UK\", \"AR\", \"FI\", \"BG\"]\n",
        "languages_to_process = {\n",
        "  \"CZ\": True,\n",
        "  \"RU\": True,\n",
        "  \"FR\": True,\n",
        "  \"DE\": True,\n",
        "  \"EN\": True,\n",
        "  \"PL\": True,\n",
        "  \"IT\": True,\n",
        "  \"JA\": True,\n",
        "  \"UK\": True,\n",
        "  \"AR\": True,\n",
        "  \"FI\": True,\n",
        "  \"BG\": True,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r5l2xqgk8N7"
      },
      "source": [
        "## Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHfk50TfuGwH"
      },
      "source": [
        "def chunkstring(string, length):\n",
        "    return re.findall('.{%d}' % length, string)\n",
        "\n",
        "def toPickleLoadList(string, textList):\n",
        "    file_name = \"texts_\" + string + \"_prep.pkl\"\n",
        "    open_file = open(file_name, \"wb\")\n",
        "    pickle.dump(textList, open_file)\n",
        "    open_file.close()\n",
        "    print(\"done \" + lang)\n",
        "\n",
        "def fromPickleLoadList(string):\n",
        "    file_name = \"texts_\" + string + \"_prep.pkl\"\n",
        "    open_file = open(file_name, \"rb\")\n",
        "    loaded_list = pickle.load(open_file)\n",
        "    open_file.close()\n",
        "    return loaded_list\n",
        "\n",
        "def chunkDataFrames(lang):\n",
        "    print(\"start \" + lang)\n",
        "    df = pd.read_pickle(\"./texts_\"+ lang + \".pkl\")\n",
        "    df = df[df['text'].apply(lambda x: len(x)>100)]\n",
        "    # df = df[~df.text.str.startswith(' redirect', na=True)]\n",
        "    df.text = df.text.replace('\\s+', ' ', regex=True)\n",
        "    df = df.text.str.cat(sep='')\n",
        "    textList = chunkstring(df, 200)\n",
        "    toPickleLoadList(lang, textList)\n",
        "    del df;  gc.collect()\n",
        "    del textList;  gc.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zoaj3NtCUsQ0"
      },
      "source": [
        "for lang in languages:\n",
        "    if languages_to_process[lang]:\n",
        "        chunkDataFrames(lang)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSAg50zD22fS"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "text_to_tokens = []\n",
        "id_lang = []\n",
        "id = 0\n",
        "delka = 100000\n",
        "\n",
        "for lang in languages:\n",
        "    df = shuffle(fromPickleLoadList(lang))\n",
        "    df = df[:delka]\n",
        "    id_lang = id_lang + [id] * delka\n",
        "    id = id + 1\n",
        "    text_to_tokens = text_to_tokens + df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUZJFcaR23sJ"
      },
      "source": [
        "data_tuples = list(zip(text_to_tokens,id_lang))\n",
        "df = pd.DataFrame(data_tuples, columns=['X','Y'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnOvuHQu3t5Z"
      },
      "source": [
        "df = shuffle(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq5-HUCm2eAN"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73LzfF4MlJK3"
      },
      "source": [
        "# Part 3: Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl-5id2CleXj"
      },
      "source": [
        "## Imports and settings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkDJCvxt8BsX"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXmkpo5X4n6I"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUO_CLzElweI"
      },
      "source": [
        "## Model creation and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkjNu2mk42cr"
      },
      "source": [
        "def simple_tokenizer(txt, min_len=2):\n",
        "    all_tokens = re.compile(r'[\\w\\d]+').findall(txt)\n",
        "    all_tokens = [x.lower() for x in all_tokens]\n",
        "    return [token for token in all_tokens if len(token) >= min_len]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqQqkh_K4oAY"
      },
      "source": [
        "sklearn_pipeline = Pipeline((('vect', TfidfVectorizer(tokenizer=simple_tokenizer,\n",
        "                                                      max_df=0.8,\n",
        "                                                      min_df=50)),\n",
        "                             ('cls', LogisticRegression())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK7ZUCei4oH3"
      },
      "source": [
        "sklearn_pipeline.fit(train['X'].to_numpy(), train['Y'].to_numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCDsuy8I4oOV"
      },
      "source": [
        "sklearn_train_pred = sklearn_pipeline.predict_proba(train['X'].to_numpy())\n",
        "sklearn_train_loss = F.cross_entropy(torch.from_numpy(sklearn_train_pred),\n",
        "                                                 torch.from_numpy(train['Y'].to_numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56-lC6Y36Hqf"
      },
      "source": [
        "print('train loss', float(sklearn_train_loss))\n",
        "print('acc', accuracy_score(train['Y'], sklearn_train_pred.argmax(-1)))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F96hEHY6Hxz"
      },
      "source": [
        "sklearn_test_pred = sklearn_pipeline.predict_proba(test['X'].to_numpy())\n",
        "sklearn_test_loss = F.cross_entropy(torch.from_numpy(sklearn_test_pred),\n",
        "                                                torch.from_numpy(test['Y'].to_numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QQfqLHY6IC3"
      },
      "source": [
        "print('Val loss', float(sklearn_test_loss))\n",
        "print('acc', accuracy_score(test['Y'], sklearn_test_pred.argmax(-1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7iRLNRL7pnC"
      },
      "source": [
        "pokus = ['hilft zu lernen oder.', 'no jo ale to neni  ', ' s a song by the', 'Существуют оба потом']\n",
        "for i in range(0, len(pokus)):\n",
        "    print(languages[sklearn_pipeline.predict_proba(pokus).argmax(-1)[i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiEP5jaz9WQg"
      },
      "source": [
        "languages = ['CZ', 'RU', 'FR', 'DE', 'EN', 'PL', 'IT', 'JA', \"UK\", \"AR\", \"FI\", \"BG\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff2Kkxp99WXe"
      },
      "source": [
        "pickle.dump(sklearn_pipeline, open(\"model.pkl\", 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jksyQrdz9WgY"
      },
      "source": [
        "loaded_model = pickle.load(open(\"model1.pkl\", 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd3ccbN22ZzU"
      },
      "source": [
        "pokus = ['hilft zu lernen oder.', 'njn jak nn se mas ', ' s a song by the',\n",
        "         'Существуют оба потом', 'Южнославянски език, написан на кирилица', 'ファベットブルガリア語キリル文字',\n",
        "         'slaavilainen kieli, joka', 'اللغة السلافية الجنوبية مكتوبة']\n",
        "for i in range(0, len(pokus)):\n",
        "    print(loaded_model.predict_proba(pokus)[i])\n",
        "    print(languages[loaded_model.predict_proba(pokus).argmax(-1)[i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6aIaJ1anTe5"
      },
      "source": [
        "## Save links for future documentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ize4ZCiho_A6"
      },
      "source": [
        "#https://github.com/jeffheaton/present/blob/master/youtube/read_wikipedia.ipynb\n",
        "# https://stackoverflow.com/questions/18854620/whats-the-best-way-to-split-a-string-into-fixed-length-chunks-and-work-with-the/18854817\n",
        "#toto je komentar pro overeni githubu ten se ma ignorovat"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}